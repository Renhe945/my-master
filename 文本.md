
	 		使用公共表达式（CTE，Common Table Expressions）的方式分步骤处理数据，在Hive中这种语法有助于使复杂查询逻辑更清晰、易读
			with t1 as (
			-- 第一步：从原始数据表date_east中选择需要的字段，并对region字段进行处理
			-- 通过concat函数将原有的region字段和一个随机数进行拼接，生成一个新的region值
			-- 这样做的目的是打破原有的数据聚集情况，使得原本大量集中在某些固定region值的数据能够分散开来
			select userid,
			order_no,
			concat(region,'-',rand()) as region,
			product_no,
			color_no,
			sale_amount,
			ts
			from date_east
			),
			t2 as (
			-- 第二步：基于t1的结果集，按照新生成的region字段进行分组，并使用count(1)函数统计每组的记录数量
			-- 此时由于第一步中对region做了打散处理，数据在这个新的分组维度下会分布得更加均匀，避免了严重的数据倾斜情况
			select region,
			count(1) as cnt
			from t1
			group by region
			),
			t3 as (
			-- 第三步：从t2的结果集中选择数据，同时使用substr函数提取新region字段的前两位作为一个新的分组依据re
			-- 这么做是因为我们最终可能还是希望基于原始region的大致分类来进行汇总统计，但又要避免直接使用原始region带来的数据倾斜问题
			-- 这里提取前两位可以根据实际业务需求灵活调整，如果region编码有特定的规则，保证能通过这种方式合理还原大致分类即可
			select region,
			substr(region,1,2) as re,
			cnt
			from t2
			)
			-- 最后一步：从t3的结果集中，按照新提取出来的re字段进行分组，并再次统计每组的记录数量
			-- 通过这样一系列的处理，实现了在避免数据倾斜的前提下，完成对数据按照地区分类的聚合统计操作，优化了查询性能
			select re,
			count(1) as cnt
			from t3
			group by re;
---------------------------------------------------------------------------
flume的断点续传：
在 Flume 中利用 offset 实现断点续传，可以采用Kafka作为Channel。
当flume将数据发送到kafka如果突然挂掉如果包含 offset 就可以从上次 offset记录的地方 继续读取

kafkaChannel是什么
KafkaChannel 是一种数据通道，用于将数据从数据源传到kafka，
并且在执行flume的时候在数据成功进入到kafkaChannel的缓存中就算出了问题，后续也会从这里恢复数据
保证了数据的完整性和一致性


---------------------------------------------------------------------------


---------------------------------------------------------------------------
·binlog为什么

·实时数仓基本做法：
·log：
·db：
# 每天背的问题
-----------------------------------------------------
hive的外部表和内部表：外部表是使用
静态分区和动态分区：（编译时）字段分区、（sql运行时进行）    字段值分区
hive自定义函数：udf;一进一出 udaf：多斤一处 udtf：一斤多出
分桶和分区的区别：一个是根据目录分区，一个是根据文件分桶
-----------------------------------------------------

hive的数据倾斜：是因为key的分布不均匀，mpe的表小，key比较集中
解决：使用驱动表的key 处理均匀表 使用mypjoin将小表数据保存到内存，使用map端完成rds

hive的数据存储格式：tsflie：默认格式：在导数据时拷贝到hdfs中
rfile：行列存储 先行后列
sqc flie：二进制可分割可压缩
ofile：是rfile的升级，增加了hive的读写性能
prqt：列式存储
avro:
-----------------------------------------------------
hive的压缩格式：
snp：不支持 分割赞比50%
lzo：支持索引的分割，压缩赞比50%
gzip:不支持分割 占比30%-40%
bzip2:支持分割   占比30%

hive的4个by：od by；全
st by:局部
dscbt by:
clst by

------------------------------------------------------------------------------

hdoop小文件：
怎么产生的：在hdoop中有大量的hdfs的元数据储存在nmnod中产生大量的小文件压垮nmnod
		      还是因为产生大量的数据没有处理就拷贝到集群，mv 没有进行设置限制
解决：		在 hdfs写入前使用combiner在map前合并，在 reduce 合并
			hadoop自带的hav对小文件进行归档
			使用 jvm 重用
			增加环形区的大小
hadoop的压缩格式：
有很多种有snpy，lzo，gzlp，bzlp2等
其中snpy速度快但不支持分片
lzo 支持大量数据支持
hadoop的job和task：
job使我们使用完整的mr的抽象封装
task是：job运行时处理阶段的实例如maptask，rdctask 他们都会有多个并发实例
---------------------------------------------------------------------------------
hadoop 的jond的作用：
两个nmnod的数据想要同步，
冗余
nonod;

---------------------------------------------------------------------------------














-----------------------------------------------------










-----------------------------------------------------
自我介绍：
我是谁？
会那些技术、平台？
擅长什么，实时、离线？
会什么语言，主要用在哪？

java的四大特性:Java 的四大特性是封装、继承、多态和抽象。
java的顶层父类：Object 类
flink顶层父类：Function 类家族








-----------------------------------------------------