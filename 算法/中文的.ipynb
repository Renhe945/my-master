{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 使用 jieba 库  ",
   "id": "954a46590ac35ab1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 分词\n",
    "import jieba\n",
    "s = \"我正在学习自然语言处理\" \n",
    "\n",
    "fh=jieba.lcut(s)# 使用lcut返回列表形式的结果 \n",
    "print(\"切割\",fh)#['我', '正在', '学习', '自然语言', '处理']\n",
    "qg=jieba.cut(s)#切割#返回生成器形式的结果\n",
    "# print(list(qg))#['我', '正在', '学习', '自然语言', '处理']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 中文文本特征提取",
   "id": "abe54222067af822"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T08:21:01.319399Z",
     "start_time": "2024-09-24T08:21:01.312068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 分词\n",
    "data = [\"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\",\n",
    "            \"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\",\n",
    "            \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\"]\n",
    "i_list=[]\n",
    "for j in data:\n",
    "    i=\" \".join(list(jieba.cut(j)))\n",
    "    i_list.append(i)#添加进去如list，add一样\n",
    "    print(\"=================>打印当前循环中的原始字符串\\n\",j)\n",
    "print(\"=================>包含了经过分词和连接处理后的所有字符串\\n\",i_list)\n",
    "transfer = CountVectorizer()#计数矢量化器#文本数据转换为词频矩阵。\n",
    "data = transfer.fit_transform(data)#转换向量\n",
    "print(\"=================>打印转换后的词频矩阵的数组形式\\n\",data.toarray())\n",
    "print(\"=================>打印经过CountVectorizer处理后得到的特征名称（即不同的词汇）\\n\",transfer.get_feature_names_out())\n",
    "\n",
    "\n",
    " "
   ],
   "id": "78f13ae338d8c0a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================>打印当前循环中的原始字符串\n",
      " 一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\n",
      "=================>打印当前循环中的原始字符串\n",
      " 我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\n",
      "=================>打印当前循环中的原始字符串\n",
      " 如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\n",
      "=================>包含了经过分词和连接处理后的所有字符串\n",
      " ['一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。', '我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。', '如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。']\n",
      "=================>打印转换后的词频矩阵的数组形式\n",
      " [[1 0 1 0 1 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 1]\n",
      " [0 1 0 1 0 1 0 0 0 0 0]]\n",
      "=================>打印经过CountVectorizer处理后得到的特征名称（即不同的词汇）\n",
      " ['一种还是一种今天很残酷' '了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系' '但绝对大部分是死在明天晚上'\n",
      " '你就不会真正了解它' '后天很美好' '如果只用一种方式了解某样事物' '我们是在看它的过去'\n",
      " '我们看到的从很远星系来的光是在几百万年之前发出的' '所以每个人不要放弃今天' '明天更残酷' '这样当我们看到宇宙时']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T09:10:51.696127Z",
     "start_time": "2024-09-24T09:10:51.688714Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "# 分词\n",
    "data = [\"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\",\n",
    "            \"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\",\n",
    "            \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\"]\n",
    "i_list=[]\n",
    "for j in data:\n",
    "    i=\" \".join(list(jieba.cut(j)))\n",
    "    i_list.append(i)#添加进去如list，add一样\n",
    "    print(\"=================>打印当前循环中的原始字符串\\n\",j)\n",
    "\n",
    "print(\"# 1.初始化tfidf对象\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#停用词\n",
    "ve=TfidfVectorizer(stop_words=[\"了解\", \"不要\", \"不会\", \"一种\"])\n",
    "data=ve.fit_transform(i_list)\n",
    "print(data.toarray)\n",
    "print(ve.get_feature_names_out())\n",
    " \n",
    "\n",
    "    \n"
   ],
   "id": "40b63ff832b7fb7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================>打印当前循环中的原始字符串\n",
      " 一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\n",
      "=================>打印当前循环中的原始字符串\n",
      " 我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\n",
      "=================>打印当前循环中的原始字符串\n",
      " 如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。\n",
      "# 1.初始化tfidf对象\n",
      "<bound method _cs_matrix.toarray of <3x33 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 34 stored elements in Compressed Sparse Row format>>\n",
      "['之前' '事物' '今天' '光是在' '几百万年' '发出' '取决于' '只用' '后天' '含义' '大部分' '如何' '如果'\n",
      " '宇宙' '我们' '所以' '放弃' '方式' '明天' '星系' '晚上' '某样' '残酷' '每个' '看到' '真正' '秘密'\n",
      " '绝对' '美好' '联系' '过去' '还是' '这样']\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TODO: ========================",
   "id": "185eb6a9803a2e57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#03-IFIDF基本使用.py\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "bd567fa3265b7ae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Classification 分类\n",
    "# Regresslon 回归\n",
    "# Clustering 聚类\n",
    "# Dimenslonality reduction 降维\n",
    "# Model Selection 模型选择\n",
    "# Preprocessing 特征工程"
   ],
   "id": "4831408a52e2fbaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# sklearn.datasets\n",
    "# \n",
    "# # 加载获取流行数据集\n",
    "# \n",
    "# datasets.load_*()\n",
    "# \n",
    "# # 获取小规模数据集，数据包含在 datasets 里\n",
    "# \n",
    "# datasets.fetch_*(data_home=None)\n",
    "# \n",
    "# 获取大规模数据集，需要从网络上下载，函数的第一个参数是 data_home，表示数据下载的目录，默认是~/scikit_learn_data\n",
    "# 小数据集\n",
    "# sklearn.datasets.load_iris()\n",
    "# \n",
    "# 加载并返回鸢尾花数据集\n",
    "# \n",
    "# sklearn.datasets.load_boston()\n",
    "# \n",
    "# 加载并返回波士顿房价数据集\n",
    "# \n",
    "# 大数据集\n",
    "# sklearn.datasets.fetch_20newsgroups(data_home=None, subset='train')\n",
    "# \n",
    "# subset: 'train' 或者 'test'， 'all' ，可选，选择要加载的数据集\n",
    "# 训练集的“训练”，测试集的“测试，两都的”全部“"
   ],
   "id": "f70804c6535190b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用\n",
    "# sklearn 数据集返回值介绍，load 和 fetch 返回的数据类型 datasets.base.Bunch(字典格式)\n",
    "# \n",
    "# data: 特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组\n",
    "# target: 标签数组，是 n_samples 的一维 numpy.ndarray 数组\n",
    "# DESCR: 数据描述\n",
    "# feature_names: 特征名，新闻数据，手写数字，回归数据集没有\n",
    "# target_names: 标签名\n",
    "\"\"\"\n",
    "@author: hwl\n",
    "@time: 2020-11-01 13:32:31\n",
    "@description:\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def datasets_demo():\n",
    "    \"\"\"\n",
    "    sklearn数据集使用\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    iris = load_iris()\n",
    "    print(\"数据集: \\n\", iris)\n",
    "    print(\"特征值：\\n\", iris.data, iris.data.shape)\n",
    "    print(\"目标值：\\n\", iris.target)\n",
    "    print(\"特征的名字：\\n\", iris.feature_names)\n",
    "    print(\"目标值的名字：\\n\", iris.target_names)\n",
    "    print(\"描述：\\n\", iris.DESCR)\n",
    "\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码1：sklearn数据集使用\n",
    "    datasets_demo()\n",
    "# 划分\n",
    "# 训练数据：用于训练，构建模型\n",
    "# \n",
    "# 测试数据：在模型检验时使用，用于评估模型是否有效\n",
    "# \n",
    "# 测试集 20%~30%\n",
    "# \n",
    "# sklearn.model_selection.train_test_split(arrays, *options)\n",
    "# \n",
    "# x 数据集的特征值\n",
    "# y 数据集的标签值\n",
    "# test_size 测试集的大小，一般为 float\n",
    "# random_state 随机数种子，不同的种子会造成不同的随机采样结果。相同的种子采样结果相同\n",
    "# return 训练集特征值，测试集特征值，训练集目标集，测试集目标集\n",
    "\"\"\"\n",
    "@author: hwl\n",
    "@time: 2020-11-01 14:22:20\n",
    "@description:\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def datasets_demo():\n",
    "    \"\"\"\n",
    "    sklearn数据集使用\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    iris = load_iris()\n",
    "    print(\"数据集: \\n\", iris)\n",
    "    print(\"特征值：\\n\", iris.data, iris.data.shape)\n",
    "    print(\"目标值：\\n\", iris.target)\n",
    "    print(\"特征的名字：\\n\", iris.feature_names)\n",
    "    print(\"目标值的名字：\\n\", iris.target_names)\n",
    "    print(\"描述：\\n\", iris.DESCR)\n",
    "\n",
    "    # 数据集划分\n",
    "    x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=22)\n",
    "    print(\"训练集特征值：\\n\", x_train, x_train.shape)\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码1：sklearn数据集使用\n",
    "    datasets_demo()"
   ],
   "id": "f9be82d38224bbf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sklearn\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# \n",
    "# pandas：一个数据读取非常方便以及基本的处理格式的工具，用于数据清洗、数据处理\n",
    "# \n",
    "# sklearn：对于特征的处理提供了强大的接口\n",
    "# \n",
    "# 特征抽取/特征提取\n",
    "# 将任意数据（如文本或图像）转换为可用于机器学习的数字特征\n",
    "# \n",
    "# 注：特征值化是为了计算机更好的去理解数据\n",
    "# \n",
    "# 字典特征提取（特征离散化）\n",
    "# 文本特征提取\n",
    "# 图像特征提取（深度学习将介绍）\n",
    "# API：sklearn.feature_extraction\n",
    "# \n",
    "# 应用 DictVectorizer 实现对类别特征进行数值化、离散化\n",
    "# 应用 CountVectorizer 实现对文本特征进行数值化\n",
    "# 应用 TfidfVectorizer 实现对文本特征进行数值化\n",
    "# 字典特征提取\n",
    "# 作用：对字典数据进行特征值化\n",
    "# \n",
    "# 类别 -> one-hot 编码（独热编码）\n",
    "\n",
    "# sklearn.feature_extraction.DictVectorizer(sparse=True,...)\n",
    "\n",
    "# DictVectorizer.fit_transform(X) #X:字典或者包含字典的迭代器返回 sparse 矩阵（稀疏矩阵 - 将非零值按位置表示出来，可以节省内存，提高加载效率）\n",
    "# DictVectorizer.inverse_transform(X) #X:array 数组或者 sparse 矩阵 返回值：转换之前数据格式\n",
    "# DictVectorizer.get_feature_names() #返回类别名称\n",
    "# 总结：对于特征当中存在类别信息的我们都会做 one-hot 编码处理\n",
    "\n",
    "\"\"\"\n",
    "@author: hwl\n",
    "@time: 2020-11-01 17:18:23\n",
    "@description:\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def dict_demo():\n",
    "    \"\"\"\n",
    "    字典特征抽取\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        {'city': '北京', 'temperature': 100},\n",
    "        {'city': '上海', 'temperature': 60},\n",
    "        {'city': '深圳', 'temperature': 30},\n",
    "    ]\n",
    "    # 1、实例化一个转换器类\n",
    "    transfer = DictVectorizer(sparse=False)\n",
    "    # 2、调用fit_transform\n",
    "    data_new = transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\", data_new)\n",
    "    print(\"特征名字:\\n\", transfer.get_feature_names())\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码2：字典特征抽取\n",
    "    dict_demo()\n",
    "\n",
    "# 应用场景：\n",
    "# \n",
    "# 数据集当中类别特征比较多\n",
    "# 本身拿到的数据就是字典类型\n",
    "# 文本特征提取\n",
    "# 特征：单词（这里也叫特征词），如果是对中文做文本特征提取，需要把词用空格格开\n",
    "# \n",
    "# 作用：对文本数据进行特征值化\n",
    "\n",
    "# CountVectorizer\n",
    "# sklearn.feature_extraction.text.CountVectorizer(stop_words=[]) #返回词频矩阵，stop_words 停用词列表，表示没什么用，就不做特征词了，可以到网上找停用词表\n",
    "# \n",
    "# CountVectorizer.fit_transform(X) #X: 文本或者包含文本字符串的可迭代对象 返回值：返回 sparse 矩阵\n",
    "# \n",
    "# CountVectorizer.inverse_transform(X) #X: array 数组或者 sparse 矩阵 返回值：转换之前数据格\n",
    "# \n",
    "# CountVectorizer.get_feature_names() #返回值：单词列表\n",
    "# \n",
    "# sklearn.feature_extraction.text.TfidfVectorizer\n",
    "\n",
    "# conda install jieba\n",
    "\"\"\"\n",
    "@author: hwl\n",
    "@time: 2020-11-02 21:27:59\n",
    "@description:\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import jieba\n",
    "\n",
    "def count_demo():\n",
    "    \"\"\"\n",
    "    文本特征提取：CountVecotrizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = [\"life is short, i like like python\", \"life is too long, i dislike python\"]\n",
    "    # 1、实例化一个转换器类\n",
    "    transfer = CountVectorizer(stop_words=[\"is\", \"too\"])\n",
    "    # 2、调用fit_transform\n",
    "    data_new = transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\", data_new.toarray())\n",
    "    print(\"特征名字:\\n\", transfer.get_feature_names())\n",
    "    return None\n",
    "\n",
    "def count_chinese_demo():\n",
    "    \"\"\"\n",
    "    中文文本特征提取：CountVecotrizer\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = [\"我 爱 北京 天安门\", \"天安门 上 太阳 升\"]\n",
    "    # 1、实例化一个转换器类\n",
    "    transfer = CountVectorizer()\n",
    "    # 2、调用fit_transform\n",
    "    data_new = transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\", data_new.toarray())\n",
    "    print(\"特征名字:\\n\", transfer.get_feature_names())\n",
    "    return None\n",
    "\n",
    "def cut_word(text):\n",
    "    \"\"\"\n",
    "    进行中文分词\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return \" \".join(list(jieba.cut(text)))\n",
    "\n",
    "def count_chinese_demo2():\n",
    "    \"\"\"\n",
    "    中文文本特征抽取，自动分词\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 将中文文本进行分词\n",
    "    data = [\"一种还是一种今天很残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\",\n",
    "            \"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\",\n",
    "            \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何装其与我们所了解的事物相联系。\"]\n",
    "    data_new = []\n",
    "    for sent in data:\n",
    "        data_new.append(cut_word(sent))\n",
    "\n",
    "    # 1、实例化一个转换器类\n",
    "    transfer = CountVectorizer()\n",
    "    # 2、调用fit_transform\n",
    "    data_final = transfer.fit_transform(data_new)\n",
    "    print(\"data_new:\\n\", data_final.toarray())\n",
    "    print(\"特征名字:\\n\", transfer.get_feature_names())\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码3：文本特征提取\n",
    "    # count_demo()\n",
    "    # 代码4：中文文本特征提取\n",
    "    # count_chinese_demo()\n",
    "    # 代码5：中文分词\n",
    "    # print(cut_word(\"我爱北京天安门\"))\n",
    "    # 代码6：中文文本特征提取，自动分词\n",
    "    count_chinese_demo2()\n",
    "# 关键词：在某一个类别的文章中，出现的次数很多，但是在其他类别的文章当中出现很少\n",
    "# \n",
    "# TfidfVectorizer\n",
    "# TF-IDF 的主要思想是：如果某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现，则认为此词或短语具有很好的类别区分能力，适合用来分类\n",
    "# \n",
    "# TF-IDF 作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度\n",
    "# \n",
    "# 公式\n",
    "# 词频（term frequency, tf）指的是某一个给定的词语在该文件中出现的频率\n",
    "# \n",
    "# 逆向文档频率（inverse document frequency, idf）是一个词语普通重要性的度量。某一特定词语的 idf, 可以由总文件数目除以包含词语之文件的数目，再将得到的商取以 10 为底的对数得到\n",
    "# \n",
    "# t\n",
    "# f\n",
    "# i\n",
    "# d\n",
    "# f\n",
    "# i\n",
    "# ,\n",
    "# j\n",
    "# =\n",
    "# t\n",
    "# f\n",
    "# i\n",
    "# ,\n",
    "# j\n",
    "# ×\n",
    "# i\n",
    "# d\n",
    "# f\n",
    "# i\n",
    "# tfidf\n",
    "# ​i,j\n",
    "# ​​ =tf\n",
    "# ​i,j\n",
    "# ​​ ×idf\n",
    "# ​i\n",
    "# ​​ \n",
    "# \n",
    "# TF_IDF 表示重要程度\n",
    "# \n",
    "# 例子\n",
    "# 有两个词“经济”，“非常” ，有 1000 篇文章的 语料库，其中 100 篇文章有“非常”，10 篇文章有“经济”\n",
    "# \n",
    "# 有两篇文章，文章 A（100 词）：10 次“经济”\n",
    "# \n",
    "# tf: 10 / 100 = 0.1\n",
    "# \n",
    "# idf: lg 1000/10 = 2\n",
    "# \n",
    "# TF-IDF: 0.1 * 2 = 0.2\n",
    "# \n",
    "# 文章 B（100 词）：10 次“非常”\n",
    "# \n",
    "# tf: 10 / 100 = 0.1\n",
    "# \n",
    "# idf: log 10 1000/100 = 1\n",
    "# \n",
    "# TF-IDF: 0.1 * 1 = 0.1\n",
    "# \n",
    "# 对数\n",
    "# 指数：2^3 = 8\n",
    "# \n",
    "# 对数：\n",
    "# \n",
    "# l\n",
    "# o\n",
    "# g\n",
    "# 2\n",
    "# 8\n",
    "# =\n",
    "# 3\n",
    "# log\n",
    "# ​2\n",
    "# ​​ 8=3\n",
    "# \n",
    "# l\n",
    "# o\n",
    "# g\n",
    "# 1\n",
    "# 0\n",
    "# 1\n",
    "# 0\n",
    "# =\n",
    "# 1\n",
    "# log\n",
    "# ​10\n",
    "# ​​ 10=1\n",
    "# \n",
    "# log 10 可以简写成 lg\n",
    "# \n",
    "# API\n",
    "# sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None,...)\n",
    "# \n",
    "# 返回词的权重矩阵\n",
    "\n",
    "# TfidfVectorizer.fit_transform(x) #X:文本或者包含文本字符串的可迭代对象 返回值：返回 sparse 矩阵\n",
    "# \n",
    "# TfidfVectorizer.inverse_transform(x) X:array 数组或者 sparse 矩阵 返回值：转换之前数据格式\n",
    "# \n",
    "# TfidfVectorizer.get_feature_names() 返回值：单词列表\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_demo():\n",
    "    \"\"\"\n",
    "    用TF—IDF的方法进行文本特征提取\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 将中文文本进行分词\n",
    "    data = [\"一种还是一种今天很残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。\",\n",
    "            \"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。\",\n",
    "            \"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何装其与我们所了解的事物相联系。\"]\n",
    "    data_new = []\n",
    "    for sent in data:\n",
    "        data_new.append(cut_word(sent))\n",
    "\n",
    "    # 1、实例化一个转换器类\n",
    "    transfer = TfidfVectorizer(stop_words=[\"一种\", \"所以\"])\n",
    "    # 2、调用fit_transform\n",
    "    data_final = transfer.fit_transform(data_new)\n",
    "    print(\"data_new:\\n\", data_final.toarray())\n",
    "    print(\"特征名字:\\n\", transfer.get_feature_names())\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码7：tfidf\n",
    "    tfidf_demo()\n",
    "# 特征预处理\n",
    "# 什么是特征预处理？\n",
    "# \n",
    "# 通过一些转换函数奖特征数据转换成更加适合算法模型的特征数据过程\n",
    "# \n",
    "# 为什么我们要进行归一化/标准化？\n",
    "# \n",
    "# 特征的单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级，容易影响(支配)目标结果，使得一些算法无法学习到其它的特征\n",
    "# \n",
    "# 我们需要用到一些方法进行无量纲化，使不同规格的数据转换到同一规格\n",
    "# \n",
    "# 归一化\n",
    "# 定义：通过对原始数据进行变换把数据映射到（默认为[0,1]）之间\n",
    "# \n",
    "# x\n",
    "# ′\n",
    "# =\n",
    "# x\n",
    "# −\n",
    "# m\n",
    "# i\n",
    "# n\n",
    "# m\n",
    "# a\n",
    "# x\n",
    "# −\n",
    "# m\n",
    "# i\n",
    "# n\n",
    "# x\n",
    "# ​′\n",
    "# ​​ =\n",
    "# ​max−min\n",
    "# ​\n",
    "# ​x−min\n",
    "# ​​ \n",
    "# \n",
    "# x\n",
    "# ′\n",
    "# ′\n",
    "# =\n",
    "# x\n",
    "# ′\n",
    "# ∗\n",
    "# (\n",
    "# m\n",
    "# x\n",
    "# −\n",
    "# m\n",
    "# i\n",
    "# )\n",
    "# +\n",
    "# m\n",
    "# i\n",
    "# x\n",
    "# ​′′\n",
    "# ​​ =x\n",
    "# ​′\n",
    "# ​​ ∗(mx−mi)+mi\n",
    "# \n",
    "# 作用于每一列，max 为一列的最大值，min 为一列的最小值 ，x''为最终结果，mx, mi 分别为指定区间值默认 mx 为 1，mi 为 0\n",
    "# \n",
    "# 方法：\n",
    "# \n",
    "# sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)...)\n",
    "# \n",
    "# MinMaxScalar.fit_transform(X) X:numpy_array 格式的数据[n_samples,n_features]\n",
    "# \n",
    "# 返回值：转换后的形状相同的 array\n",
    "# \n",
    "# 注意最大值最小值是变化的，另外，最大值与最小值非常容易受异常点影响，所以这种方法鲁棒性较差，只适合传统精确小数据场景。\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "def minmax_demo():\n",
    "    \"\"\"\n",
    "    归一化\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 1、获取数据\n",
    "    data = pd.read_csv('dating.txt')\n",
    "    print(\"data:\\n\", data)\n",
    "    data = data.iloc[:, :3]\n",
    "    print(\"data:\\n\", data)\n",
    "\n",
    "    # 2、实例化一个转换器类\n",
    "    transfer = MinMaxScaler(feature_range=[2, 3])\n",
    "    # 3、调用fit_transform\n",
    "    data_new = transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\", data_new)\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码8：归一化\n",
    "    minmax_demo()\n",
    "# 标准化\n",
    "# 定义：通过对原始数据进行变换把数据变换到均值为 0，标准差为 1 范围内\n",
    "# \n",
    "# x\n",
    "# ′\n",
    "# =\n",
    "# x\n",
    "# −\n",
    "# m\n",
    "# e\n",
    "# a\n",
    "# n\n",
    "# σ\n",
    "# x\n",
    "# ​′\n",
    "# ​​ =\n",
    "# ​σ\n",
    "# ​\n",
    "# ​x−mean\n",
    "# ​​ \n",
    "# \n",
    "# 作用于每一列，mean 为平均值，σ 为标准差\n",
    "# \n",
    "# 方法：\n",
    "\n",
    "sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# 处理之后，对每列来说，所有数据都聚集在均值为 0 附近，标准差为 1\n",
    "\n",
    "# StandardScaler.fit_transform(X) X:numpy array 格式的数据[n_samples,n_features]\n",
    "# \n",
    "# 返回值：转换后的形状相同的 array\n",
    "# \n",
    "# 在已有样本足够多的情况下比较稳定，适合现代嘈杂大数据场景\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "def stand_demo():\n",
    "    \"\"\"\n",
    "    标准化\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 1、获取数据\n",
    "    data = pd.read_csv('dating.txt')\n",
    "    print(\"data:\\n\", data)\n",
    "    data = data.iloc[:, :3]\n",
    "    print(\"data:\\n\", data)\n",
    "\n",
    "    # 2、实例化一个转换器类\n",
    "    transfer = StandardScaler()\n",
    "    # 3、调用fit_transform\n",
    "    data_new = transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\", data_new)\n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码9：标准化\n",
    "    stand_demo()\n",
    "# 特征降维\n",
    "# 降维是指在某些限定条件下，降低随机变量（特征）个数，得到一组“不相关”主变量的过程\n",
    "# \n",
    "# 降维的两种方法\n",
    "# \n",
    "# 特征选择\n",
    "# 主成分分析（可以理解一种特征提取的方式）\n",
    "# 特征选择\n",
    "# 数据中包含冗余或相关变量（或称特征、属性、指标等），旨在从原有特征中找出主要特征\n",
    "# \n",
    "# 方法\n",
    "# Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联\n",
    "# \n",
    "# 方差选择法：低方差特征过滤\n",
    "# \n",
    "# 相关系数：特征与特征之间的相关程度\n",
    "# \n",
    "# Embedded(嵌入式)：算法自动选择特征（特征与目标值之间的关联）\n",
    "# \n",
    "# 决策树：信息熵、信息增益\n",
    "# \n",
    "# 正则化：L1、L2\n",
    "# \n",
    "# 深度学习：卷积等\n",
    "# \n",
    "# 模块\n",
    "# sklearn.feature_selection\n",
    "# \n",
    "# 过滤式\n",
    "# 低方差特征过滤\n",
    "# 删除低方差的一些特征。再结合方差的大小来考虑这个方式的角度\n",
    "# \n",
    "# 特征方差小：某个特征大多样本的值比较相近\n",
    "# 特征方差大：某个特征很多样本的值都有差别\n",
    "# API\n",
    "\n",
    "# sklearn.feature_selection.VarianceThreshold(threshold = 0.0)\n",
    "\n",
    "# 删除所有低方差特征\n",
    "\n",
    "# Variance.fit_transform(X)\n",
    "\n",
    "# X:numpy array 格式的数据[n_samples,n_features]\n",
    "\n",
    "# 返回值：训练集差异低于 threshold 的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。\n",
    "\n",
    "# 相关系数\n",
    "# # 皮尔逊相关系数(Pearson Correlation Coefficient) 反映变量之间相关关系密切程序的统计指标\n",
    "# \n",
    "# 公式\n",
    "# \n",
    "# r\n",
    "# =\n",
    "# n\n",
    "# ∑\n",
    "# x\n",
    "# y\n",
    "# −\n",
    "# ∑\n",
    "# x\n",
    "# ∑\n",
    "# y\n",
    "# n\n",
    "# ∑\n",
    "# x\n",
    "# 2\n",
    "# −\n",
    "# (\n",
    "# ∑\n",
    "# x\n",
    "# )\n",
    "# 2\n",
    "# n\n",
    "# ∑\n",
    "# y\n",
    "# 2\n",
    "# −\n",
    "# (\n",
    "# ∑\n",
    "# y\n",
    "# )\n",
    "# 2\n",
    "# r=\n",
    "# ​√\n",
    "# ​n∑x\n",
    "# ​2\n",
    "# ​​ −(∑x)\n",
    "# ​2\n",
    "# ​​ \n",
    "# ​\n",
    "# ​​ √\n",
    "# ​n∑y\n",
    "# ​2\n",
    "# ​​ −(∑y)\n",
    "# ​2\n",
    "# ​​ \n",
    "# ​\n",
    "# ​​ \n",
    "# ​\n",
    "# ​n∑xy−∑x∑y\n",
    "# ​​ \n",
    "# \n",
    "# 特点\n",
    "# \n",
    "# 相关系数的值介于-1 与+1 之间，即-1 <= r <= +1。其性质如下：\n",
    "# \n",
    "# 当 r>0 时，表示两变量正相关，r<0 时，两变量为负相关\n",
    "# 当|r|=1 时，表示两变量为完全相关，当 r=0 时，表示两变量间无相关关系\n",
    "# 当 0<|r|<1 时，表示两变量存在一定程序的相关。且|r|越接近 1，两变量间线性关系越密切；|r|越接近于 0，表示两变量的线性相关越弱\n",
    "# 一般可按三级划分：|r|<0.4 为低度相关；0.4<=r<0.7 为显著性相关；0.7<=|r|<1 为高度线性相关\n",
    "# API\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# x: (N,) array_like y: (N,) array_like\n",
    "# Returns: (Pearson's correlation coefficient, p-value)\n",
    "# \"\"\"\n",
    "# @author: hwl\n",
    "# @time: 2020-11-19 21:37:36\n",
    "# @description: 低方差特征过滤 相关系数\n",
    "# \"\"\"\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "def variance_demo():\n",
    "    \"\"\"\n",
    "    过滤低方差特征\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 1、获取数据\n",
    "    data = pd.read_csv(\"factor_returns.csv\")\n",
    "    data = data.iloc[:, 1:-2]\n",
    "    print(\"data:\\n\", data)\n",
    "    # 2、实例化一个转换器类\n",
    "    transfer = VarianceThreshold(threshold=10)\n",
    "    # 3、调用fit_transform\n",
    "    data_new = transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\", data_new, data_new.shape)\n",
    "\n",
    "    # 计算某两个变量之间的相关系数\n",
    "    r = pearsonr(data[\"pe_ratio\"], data[\"pb_ratio\"])\n",
    "    print(\"相关系数：\\n\", r)\n",
    "\n",
    "    return None\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码10：低方差特征过滤\n",
    "    variance_demo()\n",
    "\n",
    "# 当特征与特征之间相关性很高时：\n",
    "# \n",
    "# 选取其中一个\n",
    "# 加权求和\n",
    "# 主成分分析\n",
    "# 主成分分析(PCA)\n",
    "# 定义：高维数据转化为低维数据的过程，在此过程中可能会舍弃原有数据、创造新的变量\n",
    "# \n",
    "# 作用：是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。\n",
    "# \n",
    "# 应用：回归分析或者聚类分析\n",
    "\n",
    "sklearn.decomposition.PCA(n_components=None)\n",
    "\n",
    "# 将数据分解为较低维数空间\n",
    "# n_components:\n",
    "# 小数：表示保留百分之多少的信息\n",
    "# 整数：减少到多少特征\n",
    "# PCA.fit_transform(X) X:numpy array 格式的数据[n_samples,n_features]\n",
    "# 返回值：转换后指定维度的 array\n",
    "\"\"\"\n",
    "@author: hwl\n",
    "@time: 2020-11-19 22:44:24\n",
    "@description: PCA降维\n",
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "def pca_demo():\n",
    "    \"\"\"\n",
    "    PCA降维\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = [[2, 8, 4, 5], [6, 3, 0, 8], [5, 4, 9, 1]]\n",
    "\n",
    "    # 1、实例化一个转换器类\n",
    "    # transfer = PCA(n_components=2)\n",
    "    transfer = PCA(n_components=0.95)\n",
    "\n",
    "    # 2、调用fit_transform\n",
    "    data_new = transfer.fit_transform(data)\n",
    "    print(\"data_new:\\n\", data_new)\n",
    "\n",
    "    return None\n",
    "if __name__ == \"__main__\":\n",
    "    # 代码11：PCA降维\n",
    "    pca_demo()\n"
   ],
   "id": "e4665ced4d7de38e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
