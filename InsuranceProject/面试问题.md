# ***\*面试问题\****

面试官您好，我叫任芳玉，目前从事大数据开发工作，已有五年以上的开发经验。我个人比较擅长 Flink、Spark 和 Hive 技术领域，在离线数仓构建方面，主要依托 Hive 和 Spark，使用频率相对较高；而实时处理场景则以 Flink 为主。开发语言方面，我精通 Java 与 Python。并且，我对各类技术平台颇为熟悉，像 CDH、CDP、TDH 等，在云平台方面，对阿里云的诸多产品运用自如。
近期，我参与了一个实时电商类项目，主要针对国内的电商行业，
在实时开发部分，我主要负责以 Flink 的 API 算子为核心的工作，对于一些简单逻辑，会运用 Flink SQL 来实现。而离线开发时，基本都是借助 Hive 或者 Spark SQL 进行。
我们项目的数据源较为多样。首先是 MySQL 数据库，它主要关联业务中心、内部系统，像 CRM 管理系统、HR 人力资源管理系统以及其他各部门的业务权限数据。其次是专门存储爬虫标签、历史数据以及运行状态的数据库。再者，业务中心采用 MongoDB 来存储日志，日志以 DOC 格式保存。最后是 Oracle（傲瑞扣） 数据库，主要用于存储财务方面的数据，涵盖业财管理、经营分析以及供应链上下游的部分整合信息。
一开始，我们采用 DataX 来搭建数据管道，主要用于离线相关的指标开发，后续出于对 checkpoint（才kpAT） 精准一次特性等因素的考量，将其替换。数据经由数据管道流入数据中台，我们构建了数仓分层架构，其中 ODS 层数据基本不做清理；DWD 层负责建设明细表明细模型、原子指标、衍生指标以及派生指标；DIM 层以维度模型为核心，区分事实与维度；DWS 层建于自身的数仓内部系统，主要围绕业务做宽表、业务逻辑指标逻辑开发，按照指标进行构建；ADS 层以报表中心为导向，服务于报表中心、业务中台以及各部门产线，该层建设在 Doris 这种分析型数据库中，用于报表开发以及功能实现。
我个人在项目中的开发职责划分明确，实时开发主要负责会员体系、零售体系以及财务体系的实时数据处理；离线开发方面，则负责会员主体域、零售主体域、商品主体域，以及链接上下游的供应链部分的开发工作。
近期我经手较多的指标，在实时方面，有最基本的 PV、UV 访问量，当前的销售额、销量、成交额、成交量，销售额、锂电人数、进店人数等，还有基于与财务系统联动分析的实时数据，这些数据的计算截止到当前时刻的任务量与计算量。
我们项目每天的数据量级约为 2T，涵盖全量与增量更新数据。其中，实时数据每天流入量大概在 800G 到 900G，包括日志与业务数据，日志数据由业务中心转发至 Kafka 主题。此外，像基于权限的管理、基于元数据的管理、基于 catalog（凯特老哥） 的管理等功能实现，我们都集成了成熟的解决方案，运行状态良好。
以上就是我对这个项目的大致介绍。——————————————————————————————————————
然后是可能要问的问题

 

 

 

# ***\*Java：\****

\> java的四大特性：封装、继承、多态、抽象
\> Java的设计模式：

（创建型）单例式，懒汉式，工厂模式，抽象工厂
（结构型）代理式，装饰器，适配器
（行为型）观察者，策略，模板
\> java的顶级父类： Object
\> java中有哪些数据结构：***\*数组（Array）\*******\*，\*******\*链表（Linked List\*******\*），\*******\*栈（Stack）\*******\*，\*******\*队列（Queue）\*******\*，\*******\*哈希表（Hash Table）\*******\*，\*******\*树（Tree）\*******\*，\*******\*堆（Heap）\*******\*；\****
\> hdfs小文件的问题： 因为数据比数据块小，所以造成的数据
\> 开窗函数的排序有哪些：


\> 排序算法有哪些：

 

1. # **Hive数据倾斜问题**

    - **造成原因** - **key分布不均匀**：在进行分组（如GROUP BY）或关联（如JOIN）操作时，某些key的数据量远远大于其他key。例如，在对用户行为数据按地区进行分组统计时，如果某个地区的用户量占比极大，就会导致数据倾斜。 - **业务数据本身特性**：如某些特殊业务场景下会产生大量相同key的数据，像电商平台在促销活动期间，热门商品的订单数据可能集中在少数几个商品ID上。 - **解决方法** - **参数调整**：可以调整Hive的相关参数，如 `hive.map.aggr` 设置为 `true`，开启map端聚合，减少shuffle的数据量。 - **SQL优化**：在SQL语句层面，使用 `GROUP BY SKEWED` 子句处理倾斜的key。例如，当知道某些key可能导致倾斜时，将这些key单独处理，先对非倾斜key进行聚合，再对倾斜key进行单独聚合，最后合并结果。 - **数据预处理**：对数据进行采样分析，识别出可能导致倾斜的key，然后对这些key进行预处理，如打散数据或者对数据进行分区，使得数据分布更加均匀。

2. 2. ## **Kafka和Hive节点数量** 

      - Kafka和Hive的节点数量取决于多种因素，如数据量、吞吐量、可用性要求等。在实际部署中，Kafka节点数量可以从3个（用于基本的容错）到十几个甚至更多，以满足大规模数据传输的需求。Hive节点数量也因集群规模和工作负载而异，包括管理节点、数据节点等多个部分，一般小型集群可能有几个节点，大型企业级集群可能有几十甚至上百个节点。 

3. 3. ## **Kafka稳定性保证** 

      - **副本机制**：Kafka通过副本机制保证数据的可靠性。每个分区可以有多个副本，分布在不同的节点上。当一个节点出现故障时，其他副本可以继续提供数据服务，确保数据的可用性。 - 例如，将分区副本数设置为3，那么即使一个节点损坏，还有两个副本可以正常工作，通过选举机制（如基于Zookeeper）选择一个新的副本作为leader来继续服务。 - **监控和报警系统**：建立完善的监控系统，实时监测Kafka的各项指标，如吞吐量、延迟、节点状态等。当出现异常情况时，如某个节点的磁盘使用率过高或者网络延迟过大，及时触发报警，以便运维人员进行处理。 - **负载均衡**：合理分配分区和消息流量，避免某个节点负载过重。可以通过工具或者配置参数，根据节点的资源情况（如CPU、内存、磁盘I/O）动态调整分区的分配，确保各个节点的负载相对均衡，提高整体稳定性。 

4. 4. ## **数据仓库分层（ODS、DIM、DWD、DWS、ADS）**

       - **ODS（操作数据存储层）**：是数据仓库的第一层，主要用于存储从各个数据源（如业务数据库、日志文件等）抽取过来的原始数据，数据几乎没有经过处理，保持了数据源的原貌，例如将电商平台的原始订单数据、用户注册数据等原封不动地存储在这里。 - **DIM（维度层）**：主要存储维度数据，是对分析主题所属的各个维度的详细描述。维度是观察数据的角度，像时间维度（年、月、日等）、地理维度（国家、城市等）、产品维度（产品类别、品牌等）。维度表数据相对稳定，变化频率较低。 - **DWD（明细数据层）**：是对ODS层数据进行清洗、转换等操作后形成的明细数据层。它以业务过程为单位进行组织，例如在电商项目中，对ODS层的原始订单数据进行清洗，去除无效数据，将日期格式统一等操作后存储在DWD层，每个订单记录都清晰详细。 - **DWS（汇总数据层）**：基于DWD层的数据进行轻度汇总得到的数据层。它主要用于提供跨主题域的汇总数据，为数据分析提供更高效的数据支持。例如，对DWD层的订单数据和用户数据进行汇总，计算每个用户的订单总数、总金额等指标存储在DWS层。 - **ADS（应用数据层）**：也称为数据应用层，是直接为业务应用提供数据服务的数据层。根据具体的业务需求，从DWS层或其他层获取数据进行进一步的加工和处理，以满足特定的业务报表、数据分析工具或者业务应用（如电商平台的销售报表、用户行为分析应用）的需求。 

5. 5. ## **维度表特征和维度层目的及好处** 

      - **维度表特征** - **数据量相对较小**：与事实表相比，维度表的数据量通常较小。例如，时间维度表可能只包含有限的年份、月份、日期等数据，产品维度表包含产品类别、品牌等信息，数据量不会像包含大量交易记录的事实表那样庞大。 - **数据相对稳定**：维度表中的数据变化频率较低。比如地理维度表中的国家和城市名称一般不会频繁改变，只有在行政区划调整等特殊情况下才会更新。 - **包含描述性信息**：用于对事实表中的数据进行描述和分类。以电商订单事实表为例，维度表可以提供订单所属的用户信息（用户维度）、商品信息（商品维度）、时间信息（时间维度）等，帮助更好地理解和分析事实表中的数据。 - **维度层目的和好处** - **方便数据管理和维护**：将维度数据集中存储在一个独立的层，便于统一管理和维护维度信息。当维度信息需要更新（如新增一个产品类别或者修改一个地理区域的名称）时，只需要在维度层进行操作，而不会影响到其他层的数据。 - **提高数据一致性**：在多个数据分析场景中，可能会用到相同的维度信息。通过维度层提供统一的维度数据，可以确保不同分析主题在使用维度数据时的一致性，避免因维度数据不一致导致的分析结果偏差。 - **支持灵活的数据分析**：维度层为数据分析提供了多个观察角度。用户可以根据不同的维度（如时间、地理、产品等）对事实表数据进行切片、切块、钻取等操作，方便从不同角度挖掘数据价值，例如分析不同时间段、不同地区、不同产品的销售情况。 6. **任务优化角度（任务跑一小时未完成）** - **数据读取优化**：检查数据的存储格式和存储位置。如果数据存储格式效率较低（如文本格式与列式存储格式相比），可以考虑转换存储格式。同时，优化数据的读取路径，减少不必要的网络传输或者磁盘I/O。 - **SQL优化**：分析任务中的SQL语句，查看是否存在复杂的嵌套查询、子查询或者关联操作。对这些复杂的SQL语句进行简化，例如通过窗口函数替代复杂的子查询，或者合理调整关联顺序来提高查询效率。 - **资源分配优化**：检查任务运行时所分配的资源，如CPU、内存、磁盘I/O等。如果资源不足，可以适当增加资源分配。同时，考虑集群中是否存在资源竞争的情况，合理安排任务的运行时间或者对任务进行优先级排序，避免资源过度竞争。 

6. ### 可以根据用户画像推荐相应的物品↓是基于用户画像来定义的：：：

7. ## **用户行为偏好定义和分群方法** 

   - **用户行为偏好定义**： - **基于行为频率**：通过统计用户对不同行为（如浏览、购买、收藏等）的频率来定义偏好。例如，用户频繁浏览某类商品（如电子产品），可以认为该用户对电子产品有较高的浏览偏好。 - **基于行为时间序列**：观察用户行为的时间顺序，如用户在某个时间段内频繁购买健身器材，结合时间因素可以定义用户在该时间段内对健身器材有购买偏好。 - **基于行为深度**：除了频率，还考虑用户在行为中的参与程度。例如，用户不仅浏览了商品详情页，还查看了评论和相关推荐，说明用户对该商品的兴趣深度较高，从而定义用户对该商品的偏好程度较高。 - **用户分群方法**： - **基于人口统计学特征**：如年龄、性别、地域等。将具有相同人口统计学特征的用户划分为一个群体，例如将年龄在20 - 30岁之间的男性用户划分为一个群体，研究他们的消费行为特点。 - **基于行为相似性**：通过计算用户行为的相似度来分群。可以使用聚类算法（如K - Means聚类），根据用户对不同产品的浏览、购买等行为的相似性将用户划分为不同的群体，每个群体内的用户行为模式较为相似。 - **基于消费能力和购买周期**：根据用户的消费金额、购买频率等因素来划分群体。例如，将高消费、高频次购买的用户划分为高价值客户群体，针对这个群体提供个性化的服务和营销活动。

8. ## **电商项目中求每个商品销量最高的前10个城市** - 

   **使用SQL实现（以Hive SQL为例）** - 假设已经有销售数据表（sales_table）包含商品ID（product_id）、城市（city）和销量（sales_volume）字段。 

```sql
sql SELECT product_id, city, sales_volume FROM ( SELECT product_id, city, sales_volume, ROW_NUMBER() OVER (PARTITION BY product_id ORDER BY sales_volume DESC) as ranking FROM sales_table ) sub WHERE ranking <= 10;
```

 这段SQL通过窗口函数 `ROW_NUMBER()` 对每个商品ID内的城市销量进行排名，按照销量降序排列，然后选择排名在前10的记录，从而得到每个商品销量最高的前10个城市。 

9. ## **Kafka一致性** - **消息复制机制**：

   Kafka通过副本（replica）来实现消息的冗余存储，保证一致性。每个分区（partition）可以有多个副本，分布在不同的broker节点上。生产者（producer）发送消息到分区的主副本（leader replica），主副本负责将消息复制到其他从副本（follower replica）。当主副本出现故障时，通过选举机制从从副本中选择一个新的主副本，保证消息的持续可用性。 - **ISR（In - Sync Replicas）机制**：Kafka维护一个与主副本同步的副本列表，即ISR。只有在ISR中的副本才能在主副本故障时参与选举。副本与主副本的同步是通过检查消息的偏移量（offset）来实现的。如果一个副本的消息偏移量与主副本的偏移量相差在一定范围内，那么这个副本被认为是同步的，属于ISR。这种机制确保了只有最新的、与主副本一致的副本才能参与选举，从而保证了消息的一致性。 - **消息传递语义保证**：Kafka支持不同的消息传递语义，如最多一次（at - most - once）、最少一次（at - least - once）和精确一次（exactly - once）。对于要求高一致性的场景，可以采用精确一次的消息传递语义。这通常需要结合Kafka的事务机制（Kafka Transactions）和幂等生产者（Idempotent Producer）来实现。幂等生产者可以保证对同一消息的多次发送，在Kafka服务端只会被接收一次；事务机制可以将多个消息的发送和消费作为一个原子操作，保证在整个消息处理过程中的一致性。

 